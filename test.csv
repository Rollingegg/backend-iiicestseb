"3rd FME Workshop on Formal Methods in Software Engineering (FormaliSE 2015)","S. Gnesi; N. Plat","NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","2","","977","978","Despite their significant advantages, formal methods are not widely used in industrial software development. Following the successful workshops we organized at ICSE 2103 in San Francisco, and ICSE 2014 in Hyderabad, we organize a third edition of the FormaliSE workshop with the main goal to promote the integration between the formal methods and the software engineering communities.","","","10.1109/ICSE.2015.313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203136","Formal methods;Software engineering","Software;Conferences;Software engineering;Security;Committees;Industries;Collaboration","","","","","","","","","","IEEE","IEEE Conferences"
"Detecting Incorrect Build Rules","N. Licker; A. Rice","University of Cambridge; University of Cambridge","2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)","","2019","","","1234","1244","Automated build systems are routinely used by software engineers to minimize the number of objects that need to be recompiled after incremental changes to the source files of a project. In order to achieve efficient and correct builds, developers must provide the build tools with dependency information between the files and modules of a project, usually expressed in a macro language specific to each build tool. In order to guarantee correctness, the authors of these tools are responsible for enumerating all the files whose contents an output depends on. Unfortunately, this is a tedious process and not all dependencies are captured in practice, which leads to incorrect builds. We automatically uncover such missing dependencies through a novel method that we call build fuzzing. The correctness of build definitions is verified by modifying files in a project, triggering incremental builds and comparing the set of changed files to the set of expected changes. These sets are determined using a dependency graph inferred by tracing the system calls executed during a clean build. We evaluate our method by exhaustively testing build rules of open-source projects, uncovering issues leading to race conditions and faulty builds in 31 of them. We provide a discussion of the bugs we detect, identifying anti-patterns in the use of the macro languages. We fix some of the issues in projects where the features of build systems allow a clean solution.","","","10.1109/ICSE.2019.00125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812082","build tools;exhaustive testing;verification","Tools;Generators;Computer bugs;Linux;C++ languages;Open source software;Kernel","graph theory;program debugging;program testing;public domain software;software quality","build definitions;changed files;dependency graph;open-source projects;software engineers;dependency information","","1","25","","","","","IEEE","IEEE Conferences"
"Unsupervised Software-Specific Morphological Forms Inference from Informal Discussions","C. Chen; Z. Xing; X. Wang","Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Res. Sch. of Comput. Sci., Australian Nat. Univ., Canberra, ACT, Australia; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore","2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)","","2017","","","450","461","Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject.","","","10.1109/ICSE.2017.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985684","abbreviation;synonym;morphological form;word embedding;Stack Overflow","Software engineering;Encyclopedias;Electronic publishing;Internet;Thesauri;Dictionaries","natural language processing;software engineering;text analysis;thesauri","unsupervised software-specific morphological forms inference;informal discussions;natural language process;software engineering tasks;NLP techniques;software-specific terms;thesaurus;domain-specific lexical rules","","4","60","","","","","IEEE","IEEE Conferences"
"ActionNet: Vision-Based Workflow Action Recognition From Programming Screencasts","D. Zhao; Z. Xing; C. Chen; X. Xia; G. Li","Australian National University; Australian National University; Monash University; Monash University; Shanghai Jiao Tong University","2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)","","2019","","","350","361","Programming screencasts have two important applications in software engineering context: study developer behaviors, information needs and disseminate software engineering knowledge. Although programming screencasts are easy to produce, they are not easy to analyze or index due to the image nature of the data. Existing techniques extract only content from screencasts, but ignore workflow actions by which developers accomplish programming tasks. This significantly limits the effective use of programming screencasts in downstream applications. In this paper, we are the first to present a novel technique for recognizing workflow actions in programming screencasts. Our technique exploits image differencing and Convolutional Neural Network (CNN) to analyze the correspondence and change of consecutive frames, based on which nine classes of frequent developer actions can be recognized from programming screencasts. Using programming screencasts from Youtube, we evaluate different configurations of our CNN model and the performance of our technique for developer action recognition across developers, working environments and programming languages. Using screencasts of developers' real work, we demonstrate the usefulness of our technique in a practical application for actionaware extraction of key-code frames in developers' work.","","","10.1109/ICSE.2019.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811922","Programming Screencast;Action Recognition;Deep learning","Programming;Microsoft Windows;Feature extraction;Software engineering;Tutorials;Mice","convolutional neural nets;feature extraction;image recognition;software engineering","programming screencasts;vision-based workflow action recognition;ActionNet;image differencing;convolutional neural network;CNN model;programming languages;software engineering;developer behaviors","","1","69","","","","","IEEE","IEEE Conferences"
"Hunting for Bugs in Code Coverage Tools via Randomized Differential Testing","Y. Yang; Y. Zhou; H. Sun; Z. Su; Z. Zuo; L. Xu; B. Xu","Nanjing University, Nanjing, China; Nanjing University, Nanjing, China; Unaffiliated; ETH Zurich, Switzerland; Nanjing University, Nanjing, China; Nanjing University, Nanjing, China; Nanjing University, Nanjing, China","2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)","","2019","","","488","499","Reliable code coverage tools are critically important as it is heavily used to facilitate many quality assurance activities, such as software testing, fuzzing, and debugging. However, little attention has been devoted to assessing the reliability of code coverage tools. In this study, we propose a randomized differential testing approach to hunting for bugs in the most widely used C code coverage tools. Specifically, by generating random input programs, our approach seeks for inconsistencies in code coverage reports produced by different code coverage tools, and then identifies inconsistencies as potential code coverage bugs. To effectively report code coverage bugs, we addressed three specific challenges: (1) How to filter out duplicate test programs as many of them triggering the same bugs in code coverage tools; (2) how to automatically reduce large test programs to much smaller ones that have the same properties; and (3) how to determine which code coverage tools have bugs? The extensive evaluations validate the effectiveness of our approach, resulting in 42 and 28 confirmed/fixed bugs for gcov and llvm-cov, respectively. This case study indicates that code coverage tools are not as reliable as it might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of code coverage tools. This work opens up a new direction in code coverage validation which calls for more attention in this area.","","","10.1109/ICSE.2019.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812045","Code Coverage;Differential Testing;Coverage Tools;Bug Detection.","Computer bugs;Tools;Software;Reliability;Fuzzing;Debugging","program debugging;program testing;software tools","random input programs;C code coverage tools;randomized differential testing;code coverage validation;potential code coverage bugs;code coverage reports;reliable code coverage tools","","2","60","","","","","IEEE","IEEE Conferences"
"When Code Completion Fails: A Case Study on Real-World Completions","V. J. Hellendoorn; S. Proksch; H. C. Gall; A. Bacchelli","UC Davis; University of Zurich; University of Zurich; University of Zurich","2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)","","2019","","","960","970","Code completion is commonly used by software developers and is integrated into all major IDE's. Good completion tools can not only save time and effort but may also help avoid incorrect API usage. Many proposed completion tools have shown promising results on synthetic benchmarks, but these benchmarks make no claims about the realism of the completions they test. This lack of grounding in real-world data could hinder our scientific understanding of developer needs and of the efficacy of completion models. This paper presents a case study on 15,000 code completions that were applied by 66 real developers, which we study and contrast with artificial completions to inform future research and tools in this area. We find that synthetic benchmarks misrepresent many aspects of real-world completions; tested completion tools were far less accurate on real-world data. Worse, on the few completions that consumed most of the developers' time, prediction accuracy was less than 20% -- an effect that is invisible in synthetic benchmarks. Our findings have ramifications for future benchmarks, tool design and real-world efficacy: Benchmarks must account for completions that developers use most, such as intra-project APIs; models should be designed to be amenable to intra-project data; and real-world developer trials are essential to quantifying performance on the least predictable completions, which are both most time-consuming and far more typical than artificial data suggests. We publicly release our preprint [https://doi.org/10.5281/zenodo.2565673] and replication data and materials [https://doi.org/10.5281/zenodo.2562249].","","","10.1109/ICSE.2019.00101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812116","Code completion;Benchmark;Language models","Tools;Benchmark testing;Data models;Context modeling;Syntactics;Vocabulary;Training","data handling;program testing;software engineering;software tools","code completion;real-world completions;software developers;synthetic benchmarks;real-world data;completion models;artificial completions;tested completion tools;tool design;real-world efficacy;real-world developer trials;predictable completions","","1","29","","","","","IEEE","IEEE Conferences"